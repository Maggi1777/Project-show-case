# -*- coding: utf-8 -*-
"""Intelligent_Feedback_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SS_1AQqQ046e0PAZ50jPy4mcD_v2kIp4
"""

# Installing Required Library
!pip install pandas
!pip install scikit-learn

#Importing Required Library
import numpy as np
import pandas as pd
import json
import pickle
import requests
import zipfile
import os

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.initializers import Constant

#Loading the Dataset
df = pd.read_csv("/content/data_sb - data_book (1) (1).csv")

df

df.columns

# Preprocess your dataset
X = df[['Compliments', 'Questions', 'Feedback']]
y = df["Summary/Takeaway"]

df.columns

X.head()

y

# Concatenate 'Compliments', 'Questions', and 'Feedback' columns into a single text
x = X['Compliments'] + ' ' + X['Questions'] + ' ' + X['Feedback']

df1 = pd.DataFrame({'Response': x, 'Summary/Takeaway': y})

df1

df = df1

#Data Preprocessing


responses = df['Response'].tolist()
summaries = df['Summary/Takeaway'].tolist()

# Combine responses and summaries for training
all_texts = responses + summaries

# Tokenize and preprocess text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_texts)

# Define vocabulary size
vocab_size = len(tokenizer.word_index) + 1

# Convert text to sequences
sequences = tokenizer.texts_to_sequences(all_texts)

padded_sequences = pad_sequences(sequences)

padded_sequences

# Create input-output pairs
input_sequences = []
for sequence in sequences:
    for i in range(1, len(sequence)):
        n_gram_sequence = sequence[:i+1]
        input_sequences.append(n_gram_sequence)

# Pad sequences for uniform length
max_sequence_length = max([len(seq) for seq in input_sequences])
padded_input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')

# Create predictors and labels
X, y = padded_input_sequences[:, :-1], padded_input_sequences[:, -1]
y = to_categorical(y, num_classes=vocab_size)

X

y

# Specify the URL of the GloVe embeddings file
glove_url = 'https://nlp.stanford.edu/data/glove.6B.zip'

# Specify the local file name for saving the downloaded ZIP file
zip_file_path = 'glove.6B.zip'

# Specify the local directory for extracting the contents
extracted_dir_path = 'glove.6B'

# Download the GloVe ZIP file
response = requests.get(glove_url, stream=True)
with open(zip_file_path, 'wb') as zip_file:
    for chunk in response.iter_content(chunk_size=128):
        zip_file.write(chunk)

# Extract the contents of the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_dir_path)

# Clean up: Remove the ZIP file
os.remove(zip_file_path)

# The embeddings file is now available in the extracted directory
embedding_file = os.path.join(extracted_dir_path, 'glove.6B.100d.txt')

# Use the embedding_file in code
embedding_dim = 100

embedding_index = {}
with open(embedding_file, 'r', encoding='utf-8') as file:
    for line in file:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embedding_index[word] = coefs

# Create embedding matrix
embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
    embedding_vector = embedding_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

# Define LSTM-based model with pre-trained embeddings
model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=embedding_dim,
                    input_length=X.shape[1],
                    embeddings_initializer=Constant(embedding_matrix),
                    trainable=False))  # Set trainable to False to use pre-trained embeddings
model.add(LSTM(300, return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(300))
model.add(Dense(vocab_size, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Display model summary
print(model.summary())

# Train the model
model.fit(X, y, epochs=35, batch_size=128, verbose=1, validation_split=0.2)

# Function which takes input and generate summary


def generate_summary(seed_text, max_length=50):
    for _ in range(max_length):
        # Tokenize the input sequence
        seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]
        # Pad the input sequence
        padded_seed_sequence = pad_sequences([seed_sequence], maxlen=max_sequence_length-1, padding='pre')
        # Predict the next word
        predicted_index = np.argmax(model.predict(padded_seed_sequence), axis=-1)
        # Convert index to word
        predicted_word = tokenizer.index_word.get(predicted_index[0], '')
        # Update the seed text for the next iteration
        seed_text += ' ' + predicted_word
        if predicted_word == '.':
            break  # Break if a period is predicted, assuming the end of a sentence
    return seed_text.split('.')[-1].strip()


# Test Data
test_data = [
    "The project's innovative approach to problem-solving is highly commendable. The use of storytelling elements in the documentation is engaging. How did you validate the performance of your ML model? Can your project be extended to include real-time streaming data? Constructive feedback on the project's impact and potential improvements. Consider discussing potential directions for future research"
]

# Generate responses
for input_text in test_data:
    generated_summary = generate_summary(input_text)
    print("\nModel Suggestion:")
    print("-", generated_summary)

# Save the model to a file
model.save('model1.h5')

# Saving the model for deployment on cloud

filename = 'sbmodel.sav'
pickle.dump(model, open(filename, 'wb'))

# loading the saved model
loaded_model = pickle.load(open('/content/sbmodel.sav', 'rb'))

#Freeze the requiremments
!pip3 freeze > requirements.txt